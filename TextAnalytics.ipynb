{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /Users/xenang/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import LabelEncoder \n","import os\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import PorterStemmer, WordNetLemmatizer\n","from nltk.corpus import wordnet\n","import string\n","nltk.download('averaged_perceptron_tagger')\n","from datetime import datetime"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["#combine Excel\n","hosp_lists = os.listdir(\"OCR\")\n","hospitalname = []\n","\n","for i in hosp_lists:\n","    sep = i.split()\n","    sep2 = sep[3].split(\".\")\n","    hospitalname.append(sep2[0])\n","hospitalname\n","\n","\n","combined_df = pd.DataFrame()\n","\n","for hospitals in hosp_lists:\n","    excel_file_path = \"OCR/\" + hospitals\n","\n","    all_sheets = pd.read_excel(excel_file_path, sheet_name=None)\n","    sep = hospitals.split()\n","    sep2 = sep[3].split(\".\")\n","    hospitalname = sep2[0]\n","\n","\n","    for sheet_name, sheet_df in all_sheets.items():\n","        sheet_df['Codes'] = sheet_name\n","        sheet_df['Hospitals'] = hospitalname\n","        try:\n","            combined_df = pd.concat([combined_df, sheet_df], ignore_index=True)\n","        except ValueError as e:\n","            print(f\"Error processing sheet '{sheet_name}': {e}\")\n","\n","ocr_excel = combined_df"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/var/folders/sp/84j13ybj0zd_p4y410zpx7jc0000gn/T/ipykernel_11863/3336145435.py:9: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  gender.loc[name.isin(['F', 'M'])] = gender[name.isin(['F', 'M'])].fillna(name)\n"]}],"source":["#normalise\n","codes = ocr_excel['Codes']\n","sn = ocr_excel['S/N']\n","gender = ocr_excel['Gender']\n","name = ocr_excel['NAME']\n","\n","ocr_excel.drop(ocr_excel[((codes == '700') & (sn == 381)) | ((gender.isna()) & (name.isna()))].index, inplace=True)\n","gender = ocr_excel['Gender']\n","gender.loc[name.isin(['F', 'M'])] = gender[name.isin(['F', 'M'])].fillna(name)\n","name = ocr_excel['NAME']\n","name.replace('F', np.NaN, inplace=True)\n","name.replace('M', np.NaN, inplace=True)\n","ocr_excel['Type'].replace('OUPATIENT', 'OUTPATIENT', inplace=True)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/var/folders/sp/84j13ybj0zd_p4y410zpx7jc0000gn/T/ipykernel_11863/1943261673.py:42: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  combined_df['Date '] = pd.to_datetime(combined_df['Date '])\n","/var/folders/sp/84j13ybj0zd_p4y410zpx7jc0000gn/T/ipykernel_11863/1943261673.py:44: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  combined_df['Year'] = combined_df['Date '].dt.year\n","/var/folders/sp/84j13ybj0zd_p4y410zpx7jc0000gn/T/ipykernel_11863/1943261673.py:45: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  combined_df['Month'] = combined_df['Date '].dt.month\n","/var/folders/sp/84j13ybj0zd_p4y410zpx7jc0000gn/T/ipykernel_11863/1943261673.py:46: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  combined_df['Day'] = combined_df['Date '].dt.day\n"]}],"source":["#combine sheets\n","for index, row in ocr_excel.iterrows():\n","    if pd.isna(row['Gender']):\n","        matching_rows = ocr_excel.loc[codes == row['Codes']]\n","        for _, matching_row in matching_rows.iterrows():\n","            if not pd.isna(matching_row['Gender']):\n","                ocr_excel.at[index, 'DOB'] = matching_row['DOB']\n","                break\n","\n","ocr_excel.loc[gender.isnull(), 'Gender'] = \\\n","    ocr_excel.loc[gender.isnull(), 'Codes'].map(\n","        ocr_excel.dropna(subset=['Gender']).set_index('Codes')['Gender'].to_dict()\n","    )\n","\n","#dob\n","def extract_year_from_date(date_string):\n","    if isinstance(date_string, int):\n","        return date_string\n","    date_obj = pd.to_datetime(date_string, errors='coerce')\n","    if not pd.isnull(date_obj):\n","        return date_obj.year\n","    else:\n","        return None\n","\n","ocr_excel['DOB'] = ocr_excel['DOB'].apply(extract_year_from_date)\n","ocr_excel['DOB'] = ocr_excel['DOB'].fillna(ocr_excel.groupby('Codes')['DOB'].transform('last'))\n","\n","\n","current_year = datetime.now().year\n","ocr_excel['Age'] = current_year - ocr_excel['DOB']\n","dob_index = ocr_excel.columns.get_loc('DOB')\n","ocr_excel.insert(dob_index + 1, 'Age', ocr_excel.pop('Age'))\n","ocr_excel['Qty'].fillna(ocr_excel['Qty'], inplace = True)\n","ocr_excel['Subtotal GST'].fillna(ocr_excel['Subtotal']*1.07, inplace = True)\n","ocr_excel['Heading'].fillna(ocr_excel['Heading'], inplace = True)\n","\n","combined_df = ocr_excel\n","\n","#Normalise Date\n","combined_df['Date '] = pd.to_datetime(combined_df['Date '], errors='coerce')\n","combined_df = combined_df.dropna(subset=['Date '])\n","combined_df['Date '] = pd.to_datetime(combined_df['Date '])\n","\n","combined_df['Year'] = combined_df['Date '].dt.year\n","combined_df['Month'] = combined_df['Date '].dt.month\n","combined_df['Day'] = combined_df['Date '].dt.day\n","combined_df = combined_df[(combined_df['Year'] >= 2016) & (combined_df['Year'] <= 2020)]"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["#creating new columns to fit in model\n","def concat_columns(row):\n","    return ' '.join(str(item) for item in row)\n","\n","combined_df['NLP'] = combined_df[['Provider', 'Heading', 'Code', 'Description', 'LOG Dx']].apply(concat_columns, axis=1)\n","\n","descp = combined_df['NLP'].tolist()\n","unique_desc = list(set(descp))\n","\n","cat = []\n","\n","for i in unique_desc:\n","    s = i.split()\n","    if s[0] not in cat:\n","        cat.append(s[0])"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# finding words end with \"OMA\"\n","get_ma_list = []\n","for i in unique_desc:\n","    s = i.split()\n","    for get_str in s:\n","       if get_str.endswith('oma') and get_str not in get_ma_list:\n","          get_ma_list.append(get_str)\n","          \n","get_ma_list = list(set([x.lower() for x in get_ma_list]))\n","get_ma_list = get_ma_list"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# finding cancer names by MOH\n","get_excel_file = 'Input/cancer_list.xlsx'\n","cancer_list = pd.read_excel(get_excel_file)\n","cancer_word_list = []\n","\n","for cancer_type in cancer_list['Cancer type'].dropna():\n","    if cancer_type not in cancer_word_list:\n","        cancer_word_list.append(cancer_type)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["#Customisation of force adding words and remove common words\n","keywords =['medullablastoma','cancer','endocrine','hormonal','immunotherapy',\"immunotherapy\", \"metastasis\", \"benign\",\n","         \"viopsy\",'leukemia' ,'leukaemia' , \"filgrastim\", \"cytology\", \"Ondansetron\",\"lymphangiosarcoma\"]\n","\n","keyword = keywords + cancer_word_list + get_ma_list\n","\n","lowercase = [str(item).lower() if isinstance(item, str) else item for item in keyword]\n","lowercase_list = lowercase\n","common_words = {'a','b', 'and', 'or', 'the','for', 'with', 'not', 'by', 'ii','iii','system','is', 'of', \n","                'in', 'on', 'at', 'to','1','2','3','4','5','6','7','8','9','12','surgery','1st','2a','3b','complex',\n","                'c','t3','t4','treatment','type','during','only','core','exam','screening','others','others ','&stoma','stoma','glaucoma','myoma',\n","                'oncocare','chemo','chemotherapy','specimen','tumour','radiosurgery','oncocare','oncooare','invasive',\n","                'radiotherapy','transplantation','oncology','forceps'}\n","\n","lowercase_list2 = [word for word in lowercase_list if word not in common_words]\n","\n","def remove_duplicates(lst):\n","    seen = set()\n","    result = []\n","    for item in lst:\n","        if item not in seen:\n","            seen.add(item)\n","            result.append(item)\n","    return result\n","\n","unique_list = remove_duplicates(lowercase_list2)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["#Text mining / model formation\n","def remove_punctuation(text):\n","    return ''.join([char for char in text if char not in string.punctuation])\n","\n","porter = PorterStemmer()\n","lemmatizer = WordNetLemmatizer()\n","\n","def process_text(text):\n","    tokens = word_tokenize(text)\n","    tokens = [remove_punctuation(token.lower()) for token in tokens if token.isalnum()]\n","    stemmed_tokens = [porter.stem(token) for token in tokens]\n","    lemmatized_tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(token)) for token in tokens]\n","    return tokens, stemmed_tokens, lemmatized_tokens\n","\n","def get_wordnet_pos(token):\n","    tag = nltk.pos_tag([token])[0][1][0].upper()\n","    tag_dict = {\"J\": wordnet.ADJ,\n","                \"N\": wordnet.NOUN,\n","                \"V\": wordnet.VERB,\n","                \"R\": wordnet.ADV}\n","    return tag_dict.get(tag, wordnet.NOUN)\n","\n","#maiking list into string so can do manipulation later\n","def concatenate_list(my_list):\n","    concatenated_string = \"\"\n","    for element in my_list:\n","        concatenated_string += str(element) + \" \"\n","    return concatenated_string\n","\n","#this code checks the words that contain in the cancer list\n","def check_keywords(tokens):\n","    cancerlist = []\n","    for token in tokens:\n","        if token in unique_list:\n","            cancerlist.append(token)\n","    return concatenate_list(cancerlist)\n","\n","\n","def is_empty(lst):\n","    if len(lst) == 0:\n","        return 0\n","    else:\n","        return 1\n","    \n","def combinecol (cola, colb):\n","    if cola == \"cancer\" and (colb != \" \" or colb != \"\" or colb.notna()):\n","        return colb\n","    elif cola == \" \" or cola == \"\":\n","        return colb\n","    else:\n","        return cola\n","    \n","def checkuniq(lst):\n","    splits = lst.split(\" \")\n","    uniques = ['cancer','specimen','chemo','tumor']\n","    if len(splits) >= 1:\n","        for i in splits:\n","            if i in get_ma_list:\n","                return i\n","            elif i in uniques:\n","                split2 = [x for x in splits if x not in uniques]\n","                if len(split2) >= 1:\n","                    #print(\"one\")\n","                    return splits[0]\n","                else:\n","                    #print(\"two\")\n","                    return split2[1]\n","            else:\n","                #print(\"three\")\n","                return splits[0]\n","            \n","    elif len(splits) == 1:\n","        #print(\"four\")\n","        return \"cancer\"\n","\n","#Text mining on medications\n","med = []\n","\n","for cancer_type in cancer_list['Active ingredient'].dropna():\n","    splits = cancer_type.split()\n","    if splits[0] not in med:\n","        med.append(cancer_type)\n","medi = [item.lower() for item in med]\n","\n","def check_keywords2(tokens):\n","    plist = [\"and\",\"511001\", \"4001000036\",\"4001000038\",\"4001000037\",'the','novena','parkway','chemotherapy',\n","             'oncocare','chemo','chemotherapy','specimen','tumour','radiosurgery','oncocare','oncooare','invasive',\n","                'radiotherapy','transplantation','oncology','forceps','clinic']\n","    counter = 0 \n","    for token in tokens:\n","        if token == \"cancer\":\n","            counters = counter - 1\n","            if tokens[counters] not in plist:\n","                return tokens[counters]\n","        counter += 1\n","\n","def check_keywords3(tokens):\n","    medilist = []\n","    for token in tokens:\n","        if token.lower() in medi:\n","            medilist.append(token)\n","    return concatenate_list(medilist)\n","\n","def normalise(norm):\n","    if norm == \"leukemia\":\n","        return \"leukaemia\"\n","    elif norm == \"esophageal\":\n","        return \"oesophageal\"\n","    elif norm == \"cholanglocarcinoma\":\n","        return \"cholangiocarcinoma\"\n","    elif norm == \"medullablastoma\":\n","        return \"medulloblastoma\"\n","    elif norm == \"tumor\":\n","        return \"tumour\"\n","    elif norm == \"ductal\":\n","        return \"breast\"\n","    elif norm == \"gluacoma\":\n","        return \"glaucoma\"\n","    else:\n","        return norm"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["combined_df['Tokens'], combined_df['Stemmed'], combined_df['Lemmatized'] = zip(*combined_df['NLP'].apply(process_text))"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/plain":["\"\\ndef check_keywords(tokens):\\n    cancerlist = []\\n    repeatlists = ['cancer','chemo','chemotherapy','biopsy']\\n    counter = 0\\n    result_list = []\\n\\n    for token in tokens:\\n        if token in unique_list:\\n            cancerlist.append(token)\\n\\n    for i in cancer_list:\\n        for r in repeatlists:\\n            if r in i:\\n                counter += 1\\n        if counter >=1:\\n            for item in cancerlist:\\n                if item not in repeatlists:\\n                    result_list.append(item)\\n\\n            return result_list\\n        else:\\n            return cancer_list\\n\""]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\"\n","def check_keywords(tokens):\n","    cancerlist = []\n","    repeatlists = ['cancer','chemo','chemotherapy','biopsy']\n","    counter = 0\n","    result_list = []\n","\n","    for token in tokens:\n","        if token in unique_list:\n","            cancerlist.append(token)\n","\n","    for i in cancer_list:\n","        for r in repeatlists:\n","            if r in i:\n","                counter += 1\n","        if counter >=1:\n","            for item in cancerlist:\n","                if item not in repeatlists:\n","                    result_list.append(item)\n","\n","            return result_list\n","        else:\n","            return cancer_list\n","\"\"\""]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["#results text mining\n","df_cleaned = combined_df\n","\n","df_cleaned['Keywords_Present'] = df_cleaned['Tokens'].apply(check_keywords)\n","df_cleaned['Keywords_Present2'] = df_cleaned['Stemmed'].apply(check_keywords)\n","df_cleaned['Keywords_Present3'] = df_cleaned['Lemmatized'].apply(check_keywords)\n","\n","df_cleaned['is_non_empty'] = df_cleaned['Keywords_Present'].apply(is_empty)\n","df_cleaned['is_non_empty2'] = df_cleaned['Keywords_Present2'].apply(is_empty)\n","df_cleaned['is_non_empty3'] = df_cleaned['Keywords_Present3'].apply(is_empty)\n","\n","df_cleaned['Cancer'] = df_cleaned['is_non_empty'] + df_cleaned['is_non_empty2'] + df_cleaned['is_non_empty3']\n","df_cleaned['testing'] = df_cleaned['Keywords_Present'] +df_cleaned['Keywords_Present2'] +df_cleaned['Keywords_Present3']\n","\n","df_cleaned['cancer2'] = df_cleaned['Tokens'].apply(check_keywords2)\n","\n","df_cleaned['MainCancer'] = df_cleaned['testing'].apply(checkuniq)\n","df_cleaned['medicine2'] = df_cleaned['Lemmatized'].apply(check_keywords3)\n","df_cleaned['Combined'] = df_cleaned.apply(lambda row: combinecol(row['MainCancer'], row['cancer2']), axis=1)\n","df_cleaned['Patient'] = df_cleaned['Hospitals'].astype(str) + \" - \"+ df_cleaned['Codes'].astype(str)\n","df_cleaned['Patient'] = df_cleaned['Patient'].str.strip()\n","df_cleaned['CancerPatient'] = df_cleaned.groupby('Patient')['Combined'].transform(lambda x: 1 if x.notna().any() else 0)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["df_cleaned['Combined'] = df_cleaned.apply(lambda row: combinecol(row['MainCancer'], row['cancer2']), axis=1)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["oldcol = []\n","for i in df_cleaned.columns:\n","    oldcol.append(i)"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["columns_to_drop = ['S/N','is_non_empty', 'is_non_empty2','is_non_empty3',\"Keywords_Present\",'Keywords_Present2','Keywords_Present3'\n","                   , \"Pt. Ref. No.\",\"NAME\",\"Invoice Period\"]\n","\n","df_cleaned = df_cleaned.drop(columns=columns_to_drop)\n","\n","#lst = ['Year', 'Month', 'Day', 'CancerPatient', 'Age', 'Combined', 'Consultant', 'medicine2', 'Total Price', 'Hospitals', 'Patient', 'Gender','Qty','']\n","# Filter DataFrame to keep only specified columns\n","#df_cleaned = df_cleaned[lst]"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Combined data has been written to Output/OCR_excel_LY.xlsx\n"]}],"source":["df_cleaned['Combined'] = df_cleaned['Combined'].apply(normalise)\n","\n","# Specify the path for the new Excel file\n","output_excel_path = 'Output/OCR_excel_LY.xlsx'\n","\n","# Write the combined dataframe to a new Excel file\n","df_cleaned.to_excel(output_excel_path, index=False)\n","\n","# Display a message indicating the success\n","print(f\"Combined data has been written to {output_excel_path}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":2}
