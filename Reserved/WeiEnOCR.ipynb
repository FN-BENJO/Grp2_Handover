{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35027598",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/xenang/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import string\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ca44986",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine Excel\n",
    "hosp_lists = os.listdir(\"OCR\")\n",
    "hospitalname = []\n",
    "\n",
    "for i in hosp_lists:\n",
    "    sep = i.split()\n",
    "    sep2 = sep[3].split(\".\")\n",
    "    hospitalname.append(sep2[0])\n",
    "hospitalname\n",
    "\n",
    "\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "for hospitals in hosp_lists:\n",
    "    excel_file_path = \"OCR/\" + hospitals\n",
    "\n",
    "    all_sheets = pd.read_excel(excel_file_path, sheet_name=None)\n",
    "    sep = hospitals.split()\n",
    "    sep2 = sep[3].split(\".\")\n",
    "    hospitalname = sep2[0]\n",
    "\n",
    "\n",
    "    for sheet_name, sheet_df in all_sheets.items():\n",
    "        sheet_df['Codes'] = sheet_name\n",
    "        sheet_df['Hospitals'] = hospitalname\n",
    "        try:\n",
    "            combined_df = pd.concat([combined_df, sheet_df], ignore_index=True)\n",
    "        except ValueError as e:\n",
    "            print(f\"Error processing sheet '{sheet_name}': {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e61e421",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "['Date']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sp/84j13ybj0zd_p4y410zpx7jc0000gn/T/ipykernel_64497/2072814220.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcombined_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, axis, how, thresh, subset, inplace, ignore_index)\u001b[0m\n\u001b[1;32m   6403\u001b[0m             \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magg_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6404\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6405\u001b[0m             \u001b[0mcheck\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6406\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6407\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6408\u001b[0m             \u001b[0magg_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magg_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6410\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mthresh\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_default\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: ['Date']"
     ]
    }
   ],
   "source": [
    "combined_df.dropna(subset=['Date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2adbc51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a67731",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare for NLP\n",
    "def concat_columns(row):\n",
    "    return ' '.join(str(item) for item in row)\n",
    "\n",
    "combined_df['NLP'] = combined_df[['Provider', 'Type', 'Heading','Description','Category','LOG Dx']].apply(concat_columns, axis=1)\n",
    "\n",
    "descp = combined_df['NLP'].tolist()\n",
    "unique_desc = list(set(descp))\n",
    "\n",
    "cat = []\n",
    "\n",
    "for i in unique_desc:\n",
    "    s = i.split()\n",
    "    if s[0] not in cat:\n",
    "        cat.append(s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ce9213",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_excel_file = 'Input/cancer_list.xlsx'\n",
    "cancer_list = pd.read_excel(get_excel_file)\n",
    "cancer_word_list = []\n",
    "\n",
    "for cancer_type in cancer_list['Cancer type'].dropna():\n",
    "    if cancer_type not in cancer_word_list:\n",
    "        cancer_word_list.append(cancer_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3068a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords =['cancer','chemo','radiotherapy','endocrine','hormonal','immunotherapy','target','biopsy',\"immunotherapy\", \"metastasis\", \"benign\",\n",
    "            \"Biopsy forceps\", \"Gastric Biopsy\",\"viopsy\", \"Biopsy Forcep\" ,\"Disposable Biopsy forceps\" ,'Core Biopsy','Radiosurgery','Oncology', \"LABORATORY \",\n",
    "            'Chemotherapy','core','specimen','leukemia' , 'leukaemia' , \"filgrastim\", \"tumour\",\"tumors\",\"tumor\", \"cytology\", \"chemotherapy-induced\", \"Ondansetron\"\n",
    "            ]\n",
    "\n",
    "keyword = keywords + cancer_word_list\n",
    "#len(keyword)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbdd619",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lowercase = [str(item).lower() if isinstance(item, str) else item for item in keyword]\n",
    "\n",
    "lowercase_list = lowercase\n",
    "\"\"\"\n",
    "lowercase_list = []\n",
    "for i in lowercase:\n",
    "    splits = str(i).split()\n",
    "    if len(splits) > 1:\n",
    "        for j in splits:\n",
    "            lowercase_list.append(j)\n",
    "    else:\n",
    "        lowercase_list.append(i)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "common_words = {'a','b', 'and', 'or', 'the','for', 'with', 'not', 'by', 'ii','iii','system','is', 'of', \n",
    "                'in', 'on', 'at', 'to','1','2','3','4','5','6','7','8','9','12','surgery','1st','2a','3b','complex',\n",
    "                'c','t3','t4','treatment','type','during','only','core','exam','screening'}\n",
    "\n",
    "# Input list\n",
    "\n",
    "# Remove common words from the input list\n",
    "lowercase_list2 = [word for word in lowercase_list if word not in common_words]\n",
    "\n",
    "def remove_duplicates(lst):\n",
    "    seen = set()\n",
    "    result = []\n",
    "    for item in lst:\n",
    "        if item not in seen:\n",
    "            seen.add(item)\n",
    "            result.append(item)\n",
    "    return result\n",
    "\n",
    "\n",
    "unique_list = remove_duplicates(lowercase_list2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8776ad78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "if \"colorectal cancer\" in unique_list:\n",
    "    print(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250c1e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    return ''.join([char for char in text if char not in string.punctuation])\n",
    "porter = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def process_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [remove_punctuation(token.lower()) for token in tokens if token.isalnum()]  # Remove punctuation and convert to lowercase\n",
    "    stemmed_tokens = [porter.stem(token) for token in tokens]\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(token)) for token in tokens]\n",
    "    return tokens, stemmed_tokens, lemmatized_tokens\n",
    "\n",
    "def get_wordnet_pos(token):\n",
    "    tag = nltk.pos_tag([token])[0][1][0].upper()  # Get the POS tag\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)  # Return NOUN if not found\n",
    "\n",
    "combined_df['Tokens'], combined_df['Stemmed'], combined_df['Lemmatized'] = zip(*combined_df['NLP'].apply(process_text))\n",
    "\n",
    "def check_keywords(tokens):\n",
    "    cancerlist = []\n",
    "    for token in tokens:\n",
    "        if token in unique_list:\n",
    "            cancerlist.append(token)\n",
    "    return cancerlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3f9a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = combined_df\n",
    "\n",
    "df_cleaned['Keywords_Present'] = df_cleaned['Tokens'].apply(check_keywords)\n",
    "df_cleaned['Keywords_Present2'] = df_cleaned['Stemmed'].apply(check_keywords)\n",
    "df_cleaned['Keywords_Present3'] = df_cleaned['Lemmatized'].apply(check_keywords)\n",
    "\n",
    "def is_empty(lst):\n",
    "    if len(lst) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "\n",
    "df_cleaned['is_non_empty'] = df_cleaned['Keywords_Present'].apply(is_empty)\n",
    "df_cleaned['is_non_empty2'] = df_cleaned['Keywords_Present2'].apply(is_empty)\n",
    "df_cleaned['is_non_empty3'] = df_cleaned['Keywords_Present3'].apply(is_empty)\n",
    "\n",
    "df_cleaned['Cancer'] = df_cleaned['is_non_empty'] + df_cleaned['is_non_empty2'] + df_cleaned['is_non_empty3'] #+ df_cleaned['is_non_empty4']\n",
    "columns_to_drop = ['is_non_empty', 'is_non_empty2','is_non_empty3','Tokens','Stemmed','Lemmatized']\n",
    "df_cleaned = df_cleaned.drop(columns=columns_to_drop)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df36717e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "group_sizes = df_cleaned.groupby('Codes')['Cancer'].sum()\n",
    "\n",
    "\n",
    "group_sizes_df = group_sizes.reset_index(name='count')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c232926c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "603"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df = df_cleaned[(df_cleaned['Codes'] == \"1140\")]\n",
    "#for i in filtered_df['NLP']:\n",
    "#    print(i)\n",
    "    \n",
    "Uniq_code = list(set(df_cleaned['Codes']))\n",
    "len(Uniq_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5ebbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in Uniq_code:\n",
    "    filtered_df = df_cleaned[(df_cleaned['Codes'] == i)]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2798305",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_sizes_df.to_excel('Output/Stats.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9548b5cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Codes</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1002</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1003</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>996</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>Template</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>603 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Codes  count\n",
       "0          10     33\n",
       "1        1000      0\n",
       "2        1001      0\n",
       "3        1002     45\n",
       "4        1003      0\n",
       "..        ...    ...\n",
       "598       996    114\n",
       "599       997      0\n",
       "600       998      0\n",
       "601       999      0\n",
       "602  Template      0\n",
       "\n",
       "[603 rows x 2 columns]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_sizes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df9e765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined data has been written to Output/OCR_excel.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Specify the path for the new Excel file\n",
    "output_excel_path = 'Output/OCR_excel.xlsx'\n",
    "\n",
    "# Write the combined dataframe to a new Excel file\n",
    "df_cleaned.to_excel(output_excel_path, index=False)\n",
    "\n",
    "# Display a message indicating the success\n",
    "print(f\"Combined data has been written to {output_excel_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
